

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>espnet nets package &mdash; ESPnet 1.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Outline" href="../tutorial.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> ESPnet
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Tutorial:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Outline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html#execution-of-example-scripts">Execution of example scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html#installation-using-docker">Installation using Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html#references">References</a></li>
</ul>
<p class="caption"><span class="caption-text">Reference Manual:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">espnet nets package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-e2e_asr">e2e_asr module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-e2e_asr_th">e2e_asr_th module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-e2e_asr_common">e2e_asr_common module</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ESPnet</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>espnet nets package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/apis/nets.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="espnet-nets-package">
<h1>espnet nets package<a class="headerlink" href="#espnet-nets-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-e2e_asr">
<span id="e2e-asr-module"></span><h2>e2e_asr module<a class="headerlink" href="#module-e2e_asr" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="e2e_asr.AttDot">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">AttDot</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>att_dim</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.AttDot" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="e2e_asr.AttDot.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.AttDot.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr.AttLoc">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">AttLoc</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>att_dim</em>, <em>aconv_chans</em>, <em>aconv_filts</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.AttLoc" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="e2e_asr.AttLoc.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.AttLoc.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr.BLSTM">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">BLSTM</code><span class="sig-paren">(</span><em>idim</em>, <em>elayers</em>, <em>cdim</em>, <em>hdim</em>, <em>dropout</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.BLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="class">
<dt id="e2e_asr.BLSTMP">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">BLSTMP</code><span class="sig-paren">(</span><em>idim</em>, <em>elayers</em>, <em>cdim</em>, <em>hdim</em>, <em>subsample</em>, <em>dropout</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.BLSTMP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="class">
<dt id="e2e_asr.CTC">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">CTC</code><span class="sig-paren">(</span><em>odim</em>, <em>eprojs</em>, <em>dropout_rate</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.CTC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="e2e_asr.CTC.log_softmax">
<code class="descname">log_softmax</code><span class="sig-paren">(</span><em>hs</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.CTC.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>log_softmax of frame activations</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>hs</strong> – </td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr.Decoder">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">Decoder</code><span class="sig-paren">(</span><em>eprojs</em>, <em>odim</em>, <em>dlayers</em>, <em>dunits</em>, <em>sos</em>, <em>eos</em>, <em>att</em>, <em>verbose=0</em>, <em>char_list=None</em>, <em>labeldist=None</em>, <em>lsm_weight=0.0</em>, <em>sampling_probability=0.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="e2e_asr.Decoder.calculate_all_attentions">
<code class="descname">calculate_all_attentions</code><span class="sig-paren">(</span><em>hs</em>, <em>ys</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.Decoder.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate all of attentions</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">list of attentions</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr.Decoder.recognize_beam">
<code class="descname">recognize_beam</code><span class="sig-paren">(</span><em>h</em>, <em>lpz</em>, <em>recog_args</em>, <em>char_list</em>, <em>rnnlm=None</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.Decoder.recognize_beam" title="Permalink to this definition">¶</a></dt>
<dd><p>beam search implementation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>h</strong> – </li>
<li><strong>recog_args</strong> – </li>
<li><strong>char_list</strong> – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr.E2E">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">E2E</code><span class="sig-paren">(</span><em>idim</em>, <em>odim</em>, <em>args</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.E2E" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="e2e_asr.E2E.calculate_all_attentions">
<code class="descname">calculate_all_attentions</code><span class="sig-paren">(</span><em>xs</em>, <em>ilens</em>, <em>ys</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.E2E.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E attention calculation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>xs_pad</strong> (<em>list</em>) – list of padded input sequences [(T1, idim), (T2, idim), …]</li>
<li><strong>ilens</strong> (<em>ndarray</em>) – batch of lengths of input sequences (B)</li>
<li><strong>ys</strong> (<em>list</em>) – list of character id sequence tensor [(L1), (L2), (L3), …]</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attention weights (B, Lmax, Tmax)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">float ndarray</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr.E2E.recognize">
<code class="descname">recognize</code><span class="sig-paren">(</span><em>x</em>, <em>recog_args</em>, <em>char_list</em>, <em>rnnlm=None</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.E2E.recognize" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E greedy/beam search</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> – </li>
<li><strong>recog_args</strong> – </li>
<li><strong>char_list</strong> – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr.Encoder">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">Encoder</code><span class="sig-paren">(</span><em>etype</em>, <em>idim</em>, <em>elayers</em>, <em>eunits</em>, <em>eprojs</em>, <em>subsample</em>, <em>dropout</em>, <em>in_channel=1</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<p>ENCODER NETWORK CLASS</p>
<p>This is the example of docstring.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>etype</strong> (<em>str</em>) – type of encoder network</li>
<li><strong>idim</strong> (<em>int</em>) – number of dimensions of encoder network</li>
<li><strong>elayers</strong> (<em>int</em>) – number of layers of encoder network</li>
<li><strong>eunits</strong> (<em>int</em>) – number of lstm units of encoder network</li>
<li><strong>epojs</strong> (<em>int</em>) – number of projection units of encoder network</li>
<li><strong>subsample</strong> (<em>str</em>) – subsampling number e.g. 1_2_2_2_1</li>
<li><strong>dropout</strong> (<em>float</em>) – dropout rate</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="e2e_asr.Loss">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">Loss</code><span class="sig-paren">(</span><em>predictor</em>, <em>mtlalpha</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="class">
<dt id="e2e_asr.NoAtt">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">NoAtt</code><a class="headerlink" href="#e2e_asr.NoAtt" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="e2e_asr.NoAtt.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.NoAtt.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr.VGG2L">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">VGG2L</code><span class="sig-paren">(</span><em>in_channel=1</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.VGG2L" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="class">
<dt id="e2e_asr.WarpCTC">
<em class="property">class </em><code class="descclassname">e2e_asr.</code><code class="descname">WarpCTC</code><span class="sig-paren">(</span><em>odim</em>, <em>eprojs</em>, <em>dropout_rate</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.WarpCTC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="e2e_asr.WarpCTC.log_softmax">
<code class="descname">log_softmax</code><span class="sig-paren">(</span><em>hs</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.WarpCTC.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>log_softmax of frame activations</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>hs</strong> – </td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="e2e_asr.linear_tensor">
<code class="descclassname">e2e_asr.</code><code class="descname">linear_tensor</code><span class="sig-paren">(</span><em>linear</em>, <em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr.linear_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply linear matrix operation only for the last dimension of a tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>linear</strong> (<em>Link</em>) – Linear link (M x N matrix)</li>
<li><strong>x</strong> (<em>Variable</em>) – Tensor (D_1 x D_2 x … x M matrix)</li>
<li><strong>y</strong> (<em>Variable</em>) – Tensor (D_1 x D_2 x … x N matrix)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-e2e_asr_th">
<span id="e2e-asr-th-module"></span><h2>e2e_asr_th module<a class="headerlink" href="#module-e2e_asr_th" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="e2e_asr_th.AttAdd">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttAdd</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>att_dim</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttAdd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Additive attention</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>att_dim</strong> (<em>int</em>) – attention dimension</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttAdd.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev</em>, <em>scaling=2.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttAdd.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttLoc forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – docoder hidden state (B x D_dec)</li>
<li><strong>att_prev</strong> (<em>torch.Tensor</em>) – dummy (does not use)</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B, D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">previous attentioin weights (B x T_max)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttAdd.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttAdd.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttCov">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttCov</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>att_dim</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttCov" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Coverage mechanism attention</p>
<dl class="docutils">
<dt>Reference: Get To The Point: Summarization with Pointer-Generator Network</dt>
<dd>(<a class="reference external" href="https://arxiv.org/abs/1704.04368">https://arxiv.org/abs/1704.04368</a>)</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>att_dim</strong> (<em>int</em>) – attention dimension</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttCov.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev_list</em>, <em>scaling=2.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttCov.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttCov forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – docoder hidden state (B x D_dec)</li>
<li><strong>att_prev_list</strong> (<em>list</em>) – list of previous attetion weight</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B, D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">list of previous attentioin weights</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttCov.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttCov.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttCovLoc">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttCovLoc</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>att_dim</em>, <em>aconv_chans</em>, <em>aconv_filts</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttCovLoc" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Coverage mechanism location aware attention</p>
<p>This attention is a combination of coverage and location-aware attentions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>att_dim</strong> (<em>int</em>) – attention dimension</li>
<li><strong>aconv_chans</strong> (<em>int</em>) – # channels of attention convolution</li>
<li><strong>aconv_filts</strong> (<em>int</em>) – filter size of attention convolution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttCovLoc.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev_list</em>, <em>scaling=2.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttCovLoc.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttCovLoc forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – docoder hidden state (B x D_dec)</li>
<li><strong>att_prev_list</strong> (<em>list</em>) – list of previous attetion weight</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B, D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">list of previous attentioin weights</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttCovLoc.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttCovLoc.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttDot">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttDot</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>att_dim</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttDot" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Dot product attention</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>att_dim</strong> (<em>int</em>) – attention dimension</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttDot.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev</em>, <em>scaling=2.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttDot.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttDot forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – dummy (does not use)</li>
<li><strong>att_prev</strong> (<em>torch.Tensor</em>) – dummy (does not use)</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B, D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">previous attentioin weight (B x T_max)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttDot.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttDot.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttForward">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttForward</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>att_dim</em>, <em>aconv_chans</em>, <em>aconv_filts</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttForward" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Forward attention</p>
<dl class="docutils">
<dt>Reference: Forward attention in sequence-to-sequence acoustic modeling for speech synthesis</dt>
<dd>(<a class="reference external" href="https://arxiv.org/pdf/1807.06736.pdf">https://arxiv.org/pdf/1807.06736.pdf</a>)</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>att_dim</strong> (<em>int</em>) – attention dimension</li>
<li><strong>aconv_chans</strong> (<em>int</em>) – # channels of attention convolution</li>
<li><strong>aconv_filts</strong> (<em>int</em>) – filter size of attention convolution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttForward.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev</em>, <em>scaling=1.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttForward.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttForward forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_hs_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – docoder hidden state (B x D_dec)</li>
<li><strong>att_prev</strong> (<em>torch.Tensor</em>) – attention weights of previous step</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B, D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">previous attentioin weights (B x T_max)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttForward.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttForward.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttForwardTA">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttForwardTA</code><span class="sig-paren">(</span><em>eunits</em>, <em>dunits</em>, <em>att_dim</em>, <em>aconv_chans</em>, <em>aconv_filts</em>, <em>odim</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttForwardTA" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Forward attention with transition agent</p>
<dl class="docutils">
<dt>Reference: Forward attention in sequence-to-sequence acoustic modeling for speech synthesis</dt>
<dd>(<a class="reference external" href="https://arxiv.org/pdf/1807.06736.pdf">https://arxiv.org/pdf/1807.06736.pdf</a>)</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eunits</strong> (<em>int</em>) – # units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>att_dim</strong> (<em>int</em>) – attention dimension</li>
<li><strong>aconv_chans</strong> (<em>int</em>) – # channels of attention convolution</li>
<li><strong>aconv_filts</strong> (<em>int</em>) – filter size of attention convolution</li>
<li><strong>odim</strong> (<em>int</em>) – output dimension</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttForwardTA.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev</em>, <em>out_prev</em>, <em>scaling=1.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttForwardTA.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttForwardTA forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B, Tmax, eunits)</li>
<li><strong>enc_hs_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – docoder hidden state (B, dunits)</li>
<li><strong>att_prev</strong> (<em>torch.Tensor</em>) – attention weights of previous step</li>
<li><strong>prev_out</strong> (<em>torch.Tensor</em>) – decoder outputs of previous step (B, odim)</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B, dunits)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">previous attentioin weights (B, Tmax)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttForwardTA.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttForwardTA.reset" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttLoc">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttLoc</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>att_dim</em>, <em>aconv_chans</em>, <em>aconv_filts</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttLoc" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>location-aware attention</p>
<dl class="docutils">
<dt>Reference: Attention-Based Models for Speech Recognition</dt>
<dd>(<a class="reference external" href="https://arxiv.org/pdf/1506.07503.pdf">https://arxiv.org/pdf/1506.07503.pdf</a>)</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>att_dim</strong> (<em>int</em>) – attention dimension</li>
<li><strong>aconv_chans</strong> (<em>int</em>) – # channels of attention convolution</li>
<li><strong>aconv_filts</strong> (<em>int</em>) – filter size of attention convolution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttLoc.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev</em>, <em>scaling=2.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttLoc.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttLoc forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – docoder hidden state (B x D_dec)</li>
<li><strong>att_prev</strong> (<em>torch.Tensor</em>) – previous attetion weight (B x T_max)</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B, D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">previous attentioin weights (B x T_max)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttLoc.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttLoc.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttLoc2D">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttLoc2D</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>att_dim</em>, <em>att_win</em>, <em>aconv_chans</em>, <em>aconv_filts</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttLoc2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>2D location-aware attention</p>
<p>This attention is an extended version of location aware attention.
It take not only one frame before attention weights, but also earlier frames into account.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>att_dim</strong> (<em>int</em>) – attention dimension</li>
<li><strong>aconv_chans</strong> (<em>int</em>) – # channels of attention convolution</li>
<li><strong>aconv_filts</strong> (<em>int</em>) – filter size of attention convolution</li>
<li><strong>att_win</strong> (<em>int</em>) – attention window size (default=5)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttLoc2D.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev</em>, <em>scaling=2.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttLoc2D.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttLoc2D forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – docoder hidden state (B x D_dec)</li>
<li><strong>att_prev</strong> (<em>torch.Tensor</em>) – previous attetion weight (B x att_win x T_max)</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B, D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">previous attentioin weights (B x att_win x T_max)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttLoc2D.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttLoc2D.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttLocRec">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttLocRec</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>att_dim</em>, <em>aconv_chans</em>, <em>aconv_filts</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttLocRec" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>location-aware recurrent attention</p>
<p>This attention is an extended version of location aware attention.
With the use of RNN, it take the effect of the history of attention weights into account.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>att_dim</strong> (<em>int</em>) – attention dimension</li>
<li><strong>aconv_chans</strong> (<em>int</em>) – # channels of attention convolution</li>
<li><strong>aconv_filts</strong> (<em>int</em>) – filter size of attention convolution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttLocRec.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev_states</em>, <em>scaling=2.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttLocRec.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttLocRec forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – docoder hidden state (B x D_dec)</li>
<li><strong>att_prev_states</strong> (<em>tuple</em>) – previous attetion weight and lstm states
((B, T_max), ((B, att_dim), (B, att_dim)))</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B, D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">previous attention weights and lstm states (w, (hx, cx))
((B, T_max), ((B, att_dim), (B, att_dim)))</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tuple</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttLocRec.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttLocRec.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttMultiHeadAdd">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttMultiHeadAdd</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>aheads</em>, <em>att_dim_k</em>, <em>att_dim_v</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadAdd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Multi head additive attention</p>
<dl class="docutils">
<dt>Reference: Attention is all you need</dt>
<dd>(<a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>)</dd>
</dl>
<p>This attention is multi head attention using additive attention for each head.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>ahead</strong> (<em>int</em>) – # heads of multi head attention</li>
<li><strong>att_dim_k</strong> (<em>int</em>) – dimension k in multi head attention</li>
<li><strong>att_dim_v</strong> (<em>int</em>) – dimension v in multi head attention</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttMultiHeadAdd.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadAdd.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttMultiHeadAdd forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – decoder hidden state (B x D_dec)</li>
<li><strong>att_prev</strong> (<em>torch.Tensor</em>) – dummy (does not use)</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B, D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">list of previous attentioin weight (B x T_max) * aheads</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttMultiHeadAdd.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadAdd.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttMultiHeadDot">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttMultiHeadDot</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>aheads</em>, <em>att_dim_k</em>, <em>att_dim_v</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadDot" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Multi head dot product attention</p>
<dl class="docutils">
<dt>Reference: Attention is all you need</dt>
<dd>(<a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>)</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>ahead</strong> (<em>int</em>) – # heads of multi head attention</li>
<li><strong>att_dim_k</strong> (<em>int</em>) – dimension k in multi head attention</li>
<li><strong>att_dim_v</strong> (<em>int</em>) – dimension v in multi head attention</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttMultiHeadDot.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadDot.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttMultiHeadDot forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – decoder hidden state (B x D_dec)</li>
<li><strong>att_prev</strong> (<em>torch.Tensor</em>) – dummy (does not use)</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B x D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">list of previous attentioin weight (B x T_max) * aheads</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttMultiHeadDot.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadDot.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttMultiHeadLoc">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttMultiHeadLoc</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>aheads</em>, <em>att_dim_k</em>, <em>att_dim_v</em>, <em>aconv_chans</em>, <em>aconv_filts</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadLoc" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Multi head location based attention</p>
<dl class="docutils">
<dt>Reference: Attention is all you need</dt>
<dd>(<a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>)</dd>
</dl>
<p>This attention is multi head attention using location-aware attention for each head.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>aheads</strong> (<em>int</em>) – # heads of multi head attention</li>
<li><strong>att_dim_k</strong> (<em>int</em>) – dimension k in multi head attention</li>
<li><strong>att_dim_v</strong> (<em>int</em>) – dimension v in multi head attention</li>
<li><strong>aconv_chans</strong> (<em>int</em>) – # channels of attention convolution</li>
<li><strong>aconv_filts</strong> (<em>int</em>) – filter size of attention convolution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttMultiHeadLoc.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev</em>, <em>scaling=2.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadLoc.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttMultiHeadLoc forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – decoder hidden state (B x D_dec)</li>
<li><strong>att_prev</strong> (<em>torch.Tensor</em>) – list of previous attentioin weight (B x T_max) * aheads</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B x D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">list of previous attentioin weight (B x T_max) * aheads</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttMultiHeadLoc.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadLoc.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.AttMultiHeadMultiResLoc">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">AttMultiHeadMultiResLoc</code><span class="sig-paren">(</span><em>eprojs</em>, <em>dunits</em>, <em>aheads</em>, <em>att_dim_k</em>, <em>att_dim_v</em>, <em>aconv_chans</em>, <em>aconv_filts</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadMultiResLoc" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Multi head multi resolution location based attention</p>
<dl class="docutils">
<dt>Reference: Attention is all you need</dt>
<dd>(<a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>)</dd>
</dl>
<p>This attention is multi head attention using location-aware attention for each head.
Furthermore, it uses different filter size for each head.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li>
<li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li>
<li><strong>aheads</strong> (<em>int</em>) – # heads of multi head attention</li>
<li><strong>att_dim_k</strong> (<em>int</em>) – dimension k in multi head attention</li>
<li><strong>att_dim_v</strong> (<em>int</em>) – dimension v in multi head attention</li>
<li><strong>aconv_chans</strong> (<em>int</em>) – maximum # channels of attention convolution
each head use #ch = aconv_chans * (head + 1) / aheads
e.g. aheads=4, aconv_chans=100 =&gt; filter size = 25, 50, 75, 100</li>
<li><strong>aconv_filts</strong> (<em>int</em>) – filter size of attention convolution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.AttMultiHeadMultiResLoc.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadMultiResLoc.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>AttMultiHeadMultiResLoc forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – decoder hidden state (B x D_dec)</li>
<li><strong>att_prev</strong> (<em>torch.Tensor</em>) – list of previous attentioin weight (B x T_max) * aheads</li>
<li><strong>scaling</strong> (<em>float</em>) – scaling parameter before applying softmax</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B x D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">list of previous attentioin weight (B x T_max) * aheads</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.AttMultiHeadMultiResLoc.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.AttMultiHeadMultiResLoc.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.BLSTM">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">BLSTM</code><span class="sig-paren">(</span><em>idim</em>, <em>elayers</em>, <em>cdim</em>, <em>hdim</em>, <em>dropout</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.BLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Bidirectional LSTM module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>idim</strong> (<em>int</em>) – dimension of inputs</li>
<li><strong>elayers</strong> (<em>int</em>) – number of encoder layers</li>
<li><strong>cdim</strong> (<em>int</em>) – number of lstm units (resulted in cdim * 2 due to biderectional)</li>
<li><strong>hdim</strong> (<em>int</em>) – number of final projection units</li>
<li><strong>subsample</strong> (<em>list</em>) – list of subsampling numbers</li>
<li><strong>dropout</strong> (<em>float</em>) – dropout rate</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.BLSTM.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>xs_pad</em>, <em>ilens</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.BLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>BLSTM forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, D)</li>
<li><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">batch of hidden state sequences (B, Tmax, erojs)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.BLSTMP">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">BLSTMP</code><span class="sig-paren">(</span><em>idim</em>, <em>elayers</em>, <em>cdim</em>, <em>hdim</em>, <em>subsample</em>, <em>dropout</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.BLSTMP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Bidirectional LSTM with projection layer module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>idim</strong> (<em>int</em>) – dimension of inputs</li>
<li><strong>elayers</strong> (<em>int</em>) – number of encoder layers</li>
<li><strong>cdim</strong> (<em>int</em>) – number of lstm units (resulted in cdim * 2 due to biderectional)</li>
<li><strong>hdim</strong> (<em>int</em>) – number of projection units</li>
<li><strong>subsample</strong> (<em>list</em>) – list of subsampling numbers</li>
<li><strong>dropout</strong> (<em>float</em>) – dropout rate</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.BLSTMP.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>xs_pad</em>, <em>ilens</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.BLSTMP.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>BLSTMP forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, idim)</li>
<li><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">batch of hidden state sequences (B, Tmax, hdim)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.CTC">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">CTC</code><span class="sig-paren">(</span><em>odim</em>, <em>eprojs</em>, <em>dropout_rate</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.CTC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>CTC module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>odim</strong> (<em>int</em>) – dimension of outputs</li>
<li><strong>eprojs</strong> (<em>int</em>) – number of encoder projection units</li>
<li><strong>dropout_rate</strong> (<em>float</em>) – dropout rate (0.0 ~ 1.0)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.CTC.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>hs_pad</em>, <em>hlens</em>, <em>ys_pad</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.CTC.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>CTC forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>hs_pad</strong> (<em>torch.Tensor</em>) – batch of padded hidden state sequences (B, Tmax, D)</li>
<li><strong>hlens</strong> (<em>torch.Tensor</em>) – batch of lengths of hidden state sequences (B)</li>
<li><strong>ys_pad</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, Lmax)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">ctc loss value</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.CTC.log_softmax">
<code class="descname">log_softmax</code><span class="sig-paren">(</span><em>hs_pad</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.CTC.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>log_softmax of frame activations</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>hs_pad</strong> (<em>torch.Tensor</em>) – 3d tensor (B, Tmax, eprojs)</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">log softmax applied 3d tensor (B, Tmax, odim)</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">torch.Tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.Decoder">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">Decoder</code><span class="sig-paren">(</span><em>eprojs</em>, <em>odim</em>, <em>dlayers</em>, <em>dunits</em>, <em>sos</em>, <em>eos</em>, <em>att</em>, <em>verbose=0</em>, <em>char_list=None</em>, <em>labeldist=None</em>, <em>lsm_weight=0.0</em>, <em>sampling_probability=0.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Decoder module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>eprojs</strong> (<em>int</em>) – # encoder projection units</li>
<li><strong>odim</strong> (<em>int</em>) – dimension of outputs</li>
<li><strong>dlayers</strong> (<em>int</em>) – # decoder layers</li>
<li><strong>dunits</strong> (<em>int</em>) – # decoder units</li>
<li><strong>sos</strong> (<em>int</em>) – start of sequence symbol id</li>
<li><strong>eos</strong> (<em>int</em>) – end of sequence symbol id</li>
<li><strong>att</strong> (<em>torch.nn.Module</em>) – attention module</li>
<li><strong>verbose</strong> (<em>int</em>) – verbose level</li>
<li><strong>char_list</strong> (<em>list</em>) – list of character strings</li>
<li><strong>labeldist</strong> (<em>ndarray</em>) – distribution of label smoothing</li>
<li><strong>lsm_weight</strong> (<em>float</em>) – label smoothing weight</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.Decoder.calculate_all_attentions">
<code class="descname">calculate_all_attentions</code><span class="sig-paren">(</span><em>hs_pad</em>, <em>hlen</em>, <em>ys_pad</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.Decoder.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate all of attentions</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>hs_pad</strong> (<em>torch.Tensor</em>) – batch of padded hidden state sequences (B, Tmax, D)</li>
<li><strong>hlens</strong> (<em>torch.Tensor</em>) – batch of lengths of hidden state sequences (B)</li>
<li><strong>ys_pad</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, Lmax)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attention weights with the following shape,
1) multi-head case =&gt; attention weights (B, H, Lmax, Tmax),
2) other case =&gt; attention weights (B, Lmax, Tmax).</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">float ndarray</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.Decoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>hs_pad</em>, <em>hlens</em>, <em>ys_pad</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.Decoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Decoder forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>hs_pad</strong> (<em>torch.Tensor</em>) – batch of padded hidden state sequences (B, Tmax, D)</li>
<li><strong>hlens</strong> (<em>torch.Tensor</em>) – batch of lengths of hidden state sequences (B)</li>
<li><strong>ys_pad</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, Lmax)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attention loss value</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">accuracy</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">float</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.Decoder.recognize_beam">
<code class="descname">recognize_beam</code><span class="sig-paren">(</span><em>h</em>, <em>lpz</em>, <em>recog_args</em>, <em>char_list</em>, <em>rnnlm=None</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.Decoder.recognize_beam" title="Permalink to this definition">¶</a></dt>
<dd><p>beam search implementation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>h</strong> (<em>torch.Tensor</em>) – encoder hidden state (T, eprojs)</li>
<li><strong>lpz</strong> (<em>torch.Tensor</em>) – ctc log softmax output (T, odim)</li>
<li><strong>recog_args</strong> (<em>Namespace</em>) – argument namespace contraining options</li>
<li><strong>char_list</strong> – list of character strings</li>
<li><strong>rnnlm</strong> (<em>torch.nn.Module</em>) – language module</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">N-best decoding results</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list of dicts</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.Decoder.zero_state">
<code class="descname">zero_state</code><span class="sig-paren">(</span><em>hs_pad</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.Decoder.zero_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.E2E">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">E2E</code><span class="sig-paren">(</span><em>idim</em>, <em>odim</em>, <em>args</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.E2E" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>E2E module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>idim</strong> (<em>int</em>) – dimension of inputs</li>
<li><strong>odim</strong> (<em>int</em>) – dimension of outputs</li>
<li><strong>args</strong> (<em>namespace</em>) – argument namespace containing options</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.E2E.calculate_all_attentions">
<code class="descname">calculate_all_attentions</code><span class="sig-paren">(</span><em>xs_pad</em>, <em>ilens</em>, <em>ys_pad</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.E2E.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E attention calculation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, idim)</li>
<li><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</li>
<li><strong>ys_pad</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, Lmax)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attention weights with the following shape,
1) multi-head case =&gt; attention weights (B, H, Lmax, Tmax),
2) other case =&gt; attention weights (B, Lmax, Tmax).</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">float ndarray</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.E2E.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>xs_pad</em>, <em>ilens</em>, <em>ys_pad</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.E2E.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, idim)</li>
<li><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</li>
<li><strong>ys_pad</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, Lmax)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">ctc loass value</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attention loss value</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">accuracy in attention decoder</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">float</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.E2E.init_like_chainer">
<code class="descname">init_like_chainer</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.E2E.init_like_chainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize weight like chainer</p>
<p>chainer basically uses LeCun way: W ~ Normal(0, fan_in ** -0.5), b = 0
pytorch basically uses W, b ~ Uniform(-fan_in**-0.5, fan_in**-0.5)</p>
<p>however, there are two exceptions as far as I know.
- EmbedID.W ~ Normal(0, 1)
- LSTM.upward.b[forget_gate_range] = 1 (but not used in NStepLSTM)</p>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.E2E.recognize">
<code class="descname">recognize</code><span class="sig-paren">(</span><em>x</em>, <em>recog_args</em>, <em>char_list</em>, <em>rnnlm=None</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.E2E.recognize" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E beam search</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>ndarray</em>) – input acouctic feature (T, D)</li>
<li><strong>recog_args</strong> (<em>namespace</em>) – argment namespace contraining options</li>
<li><strong>char_list</strong> (<em>list</em>) – list of characters</li>
<li><strong>rnnlm</strong> (<em>torch.nn.Module</em>) – language model module</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">N-best decoding results</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.Encoder">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">Encoder</code><span class="sig-paren">(</span><em>etype</em>, <em>idim</em>, <em>elayers</em>, <em>eunits</em>, <em>eprojs</em>, <em>subsample</em>, <em>dropout</em>, <em>in_channel=1</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Encoder module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>etype</strong> (<em>str</em>) – type of encoder network</li>
<li><strong>idim</strong> (<em>int</em>) – number of dimensions of encoder network</li>
<li><strong>elayers</strong> (<em>int</em>) – number of layers of encoder network</li>
<li><strong>eunits</strong> (<em>int</em>) – number of lstm units of encoder network</li>
<li><strong>epojs</strong> (<em>int</em>) – number of projection units of encoder network</li>
<li><strong>subsample</strong> (<em>list</em>) – list of subsampling numbers</li>
<li><strong>dropout</strong> (<em>float</em>) – dropout rate</li>
<li><strong>in_channel</strong> (<em>int</em>) – number of input channels</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.Encoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>xs_pad</em>, <em>ilens</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.Encoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Encoder forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, D)</li>
<li><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">batch of hidden state sequences (B, Tmax, erojs)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.Loss">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">Loss</code><span class="sig-paren">(</span><em>predictor</em>, <em>mtlalpha</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Multi-task learning loss module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>predictor</strong> (<em>torch.nn.Module</em>) – E2E model instance</li>
<li><strong>mtlalpha</strong> (<em>float</em>) – mtl coefficient value (0.0 ~ 1.0)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.Loss.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>xs_pad</em>, <em>ilens</em>, <em>ys_pad</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.Loss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-task learning loss forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, idim)</li>
<li><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</li>
<li><strong>ys_pad</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, Lmax)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">loss value</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.NoAtt">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">NoAtt</code><a class="headerlink" href="#e2e_asr_th.NoAtt" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>No attention</p>
<dl class="method">
<dt id="e2e_asr_th.NoAtt.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>enc_hs_pad</em>, <em>enc_hs_len</em>, <em>dec_z</em>, <em>att_prev</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.NoAtt.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>NoAtt forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B, T_max, D_enc)</li>
<li><strong>enc_h_len</strong> (<em>list</em>) – padded encoder hidden state lenght (B)</li>
<li><strong>dec_z</strong> (<em>torch.Tensor</em>) – dummy (does not use)</li>
<li><strong>att_prev</strong> (<em>torch.Tensor</em>) – dummy (does not use)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">attentioin weighted encoder state (B, D_enc)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">torch.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">previous attentioin weights</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="e2e_asr_th.NoAtt.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.NoAtt.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.Reporter">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">Reporter</code><span class="sig-paren">(</span><em>**links</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.Reporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="e2e_asr_th.Reporter.report">
<code class="descname">report</code><span class="sig-paren">(</span><em>loss_ctc</em>, <em>loss_att</em>, <em>acc</em>, <em>mtl_loss</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.Reporter.report" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="e2e_asr_th.VGG2L">
<em class="property">class </em><code class="descclassname">e2e_asr_th.</code><code class="descname">VGG2L</code><span class="sig-paren">(</span><em>in_channel=1</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.VGG2L" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>VGG-like module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_channel</strong> (<em>int</em>) – number of input channels</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="e2e_asr_th.VGG2L.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>xs_pad</em>, <em>ilens</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.VGG2L.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>VGG2L forward</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, D)</li>
<li><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">batch of padded hidden state sequences (B, Tmax // 4, 128)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="e2e_asr_th.make_pad_mask">
<code class="descclassname">e2e_asr_th.</code><code class="descname">make_pad_mask</code><span class="sig-paren">(</span><em>lengths</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.make_pad_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to make mask tensor containing indices of padded part</p>
<dl class="docutils">
<dt>e.g.: lengths = [5, 3, 2]</dt>
<dd><dl class="first last docutils">
<dt>mask = [[0, 0, 0, 0 ,0],</dt>
<dd>[0, 0, 0, 1, 1],
[0, 0, 1, 1, 1]]</dd>
</dl>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>lengths</strong> (<em>list</em>) – list of lengths (B)</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">mask tensor containing indices of padded part (B, Tmax)</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">torch.Tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="e2e_asr_th.pad_list">
<code class="descclassname">e2e_asr_th.</code><code class="descname">pad_list</code><span class="sig-paren">(</span><em>xs</em>, <em>pad_value</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.pad_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to pad values</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>xs</strong> (<em>list</em>) – list of torch.Tensor [(L_1, D), (L_2, D), …, (L_B, D)]</li>
<li><strong>pad_value</strong> (<em>float</em>) – value for padding</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">padded tensor (B, Lmax, D)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="e2e_asr_th.th_accuracy">
<code class="descclassname">e2e_asr_th.</code><code class="descname">th_accuracy</code><span class="sig-paren">(</span><em>pad_outputs</em>, <em>pad_targets</em>, <em>ignore_label</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.th_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to calculate accuracy</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>pad_outputs</strong> (<em>torch.Tensor</em>) – prediction tensors (B*Lmax, D)</li>
<li><strong>pad_targets</strong> (<em>torch.Tensor</em>) – target tensors (B, Lmax, D)</li>
<li><strong>ignore_label</strong> (<em>int</em>) – ignore label id</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Retrun:</th><td class="field-body"><p class="first">accuracy value (0.0 - 1.0)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">float</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="e2e_asr_th.to_cuda">
<code class="descclassname">e2e_asr_th.</code><code class="descname">to_cuda</code><span class="sig-paren">(</span><em>m</em>, <em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_th.to_cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to send tensor into corresponding device</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>m</strong> (<em>torch.nn.Module</em>) – torch module</li>
<li><strong>x</strong> (<em>torch.Tensor</em>) – torch tensor</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">torch tensor located in the same place as torch module</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">torch.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-e2e_asr_common">
<span id="e2e-asr-common-module"></span><h2>e2e_asr_common module<a class="headerlink" href="#module-e2e_asr_common" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="e2e_asr_common.end_detect">
<code class="descclassname">e2e_asr_common.</code><code class="descname">end_detect</code><span class="sig-paren">(</span><em>ended_hyps</em>, <em>i</em>, <em>M=3</em>, <em>D_end=-10.0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_common.end_detect" title="Permalink to this definition">¶</a></dt>
<dd><p>End detection</p>
<p>desribed in Eq. (50) of S. Watanabe et al
“Hybrid CTC/Attention Architecture for End-to-End Speech Recognition”</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>ended_hyps</strong> – </li>
<li><strong>i</strong> – </li>
<li><strong>M</strong> – </li>
<li><strong>D_end</strong> – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="e2e_asr_common.get_vgg2l_odim">
<code class="descclassname">e2e_asr_common.</code><code class="descname">get_vgg2l_odim</code><span class="sig-paren">(</span><em>idim</em>, <em>in_channel=3</em>, <em>out_channel=128</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_common.get_vgg2l_odim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="e2e_asr_common.label_smoothing_dist">
<code class="descclassname">e2e_asr_common.</code><code class="descname">label_smoothing_dist</code><span class="sig-paren">(</span><em>odim</em>, <em>lsm_type</em>, <em>transcript=None</em>, <em>blank=0</em><span class="sig-paren">)</span><a class="headerlink" href="#e2e_asr_common.label_smoothing_dist" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtain label distribution for loss smoothing</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>odim</strong> – </li>
<li><strong>lsm_type</strong> – </li>
<li><strong>blank</strong> – </li>
<li><strong>transcript</strong> – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../tutorial.html" class="btn btn-neutral" title="Outline" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Shinji Watanabe.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'1.0',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>